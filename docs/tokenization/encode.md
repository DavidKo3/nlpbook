---
layout: default
title: Tokenization Tutorial
parent: Preprocess
nav_order: 4
---

# Tokenization Tutorial
{: .no_toc }

이 장에서는 문장을 토큰화하고 해당 토큰들을 모델의 입력으로 만드는 과정을 튜토리얼 방식으로 소개합니다.
{: .fs-4 .ls-1 .code-example }

1. TOC
{:toc}

---

## GPT 입력값 만들기

GPT 입력값을 만들려면 토크나이저부터 준비해야 합니다. 코드1을 수행하면 GPT 모델이 사용하는 토크나이저를 초기화할 수 있습니다. `save_path`에는 GPT용 BPE 어휘 집합(`vocab.json`)과 바이그램 쌍의 병합 우선순위(`merge.txt`)가 있어야 합니다. 누군가가 구축해 놓은 결과를 사용해도 되고요, 독자 여러분이 가진 말뭉치로 직접 만든 걸 써도 됩니다. 후자처럼 하고 싶으시다면 [이전 장](https://ratsgo.github.io/nlpbook/docs/tokenization/vocab)을 참고하세요.

## **코드1** GPT 토크나이저 선언
{: .no_toc .text-delta } 
```python
from transformers import GPT2Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained(save_path)
tokenizer.pad_token = "[PAD]"
```

예시 문장 세 개를 토큰화하는 코드는 코드2입니다. 그 결과는 표1과 같습니다.

## **코드2** GPT 토크나이저로 토큰화하기
{: .no_toc .text-delta } 
```python
sentences = [
    "아 더빙.. 진짜 짜증나네요 목소리",
    "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나",
    "별루 였다..",
]
tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]
```

표1을 보면 토큰들이 알 수 없는 문자열로 구성돼 있음을 확인할 수 있습니다. 그도 그럴 것이 GPT 모델은 바이트 레벨 BPE를 적용하기 때문인데요. 문장들을 유니코드 바이트로 변환한 뒤 BPE를 수행합니다.

## **표1** GPT 토크나이저 토큰화 결과
{: .no_toc .text-delta } 

|구분|토큰1|토큰2|토큰3|토큰4|토큰5|토큰6|토큰7|토큰8|토큰9|토큰10|토큰11|토큰12|토큰13|토큰14|토큰15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|문장1|ìķĦ|ĠëįĶë¹Ļ|..|Ġì§Ħì§ľ|Ġì§ľì¦ĿëĤĺ|ëĦ¤ìļĶ|Ġëª©ìĨĮë¦¬|||||||||
|문장2|íĿł|...|íı¬ìĬ¤íĦ°|ë³´ê³ł|Ġì´ĪëĶ©|ìĺģíĻĶ|ì¤Ħ|....|ìĺ¤ë²Ħ|ìĹ°ê¸°|ì¡°ì°¨|Ġê°Ģë³į|ì§Ģ|ĠìķĬ|êµ¬ëĤĺ|
|문장3|ë³Ħë£¨|Ġìĺ|Ģëĭ¤|..||||||||||||

코드2와 표1은 GPT 토크나이저의 토큰화 결과를 살짝 맛보기 위해 설명한 것인데요. 실제 모델 입력값은 코드3으로 만듭니다.

## **코드3** GPT 모델 입력 만들기
{: .no_toc .text-delta } 
```python
batch_inputs = tokenizer(
    sentences,
    padding="max_length",
    max_length=12,
    truncation=True,
)
```

코드3 실행 결과로 두 가지의 입력값이 만들어집니다. 하나는 `input_ids`입니다. `input_ids`는 표1의 토큰화 결과를 가지고 각 토큰들을 인덱스(index)로 바꾼 것입니다. 어휘 집합(`vocab.json`)을 확인해 보면 각 어휘가 특정 정수(integer)로 매핑되어 있는 걸 확인할 수 있는데요. 여기서 참고해 인덱스로 변환한 것임을 알 수 있습니다. 표2와 같습니다.

## **표2** GPT input_ids
{: .no_toc .text-delta } 

|구분|토큰1|토큰2|토큰3|토큰4|토큰5|토큰6|토큰7|토큰8|토큰9|토큰10|토큰11|토큰12|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|문장1|334|2338|263|581|4055|464|3808|0|0|0|0|0|
|문장2|3693|336|2876|758|2883|356|806|422|9875|875|2960|7292|
|문장3|4957|451|3653|263|0|0|0|0|0|0|0|0|

표2를 자세히 보시면 모든 문장의 길이가 12로 맞춰진걸 볼 수 있습니다. 코드3에서 `max_length` 인자에 12를 넣었기 때문인데요. 이보다 짧은 문장1과 문장3은 뒤에 `[PAD]` 토큰에 해당하는 인덱스 0이 덧붙여져 있습니다. `[PAD]` 토큰은 일종의 더미 토큰으로 길이를 맞춰주는 역할을 합니다. 문장2는 원래 토큰 길이가 15였는데 12로 줄었습니다. `truncation` 옵션을 `True`로 줬기 때문입니다.

코드3 실행 결과로 `attention_mask`도 만들어졌습니다. `attention_mask`는 일반 토큰이 자리한 곳(=1)과 패딩 토큰이 자리한 곳(=0)을 구분해 알려주는 장치입니다. 표3과 같습니다.

## **표3** GPT attention_mask
{: .no_toc .text-delta } 

|구분|토큰1|토큰2|토큰3|토큰4|토큰5|토큰6|토큰7|토큰8|토큰9|토큰10|토큰11|토큰12|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|문장1|1|1|1|1|1|1|1|0|0|0|0|0|
|문장2|1|1|1|1|1|1|1|1|1|1|1|1|
|문장3|1|1|1|1|0|0|0|0|0|0|0|0|


---

## BERT 입력값 만들기

이번엔 BERT 모델의 입력값을 만들어보겠습니다. 코드4를 수행하면 BERT 모델이 사용하는 토크나이저를 초기화할 수 있습니다. `save_path`에는 BERT용 워드피스 어휘 집합(`vocab.txt`)이 있어야 합니다. 누군가가 구축해 놓은 결과를 사용해도 되고요, 독자 여러분이 가진 말뭉치로 직접 만든 걸 써도 됩니다. 후자처럼 하고 싶으시다면 [이전 장](https://ratsgo.github.io/nlpbook/docs/tokenization/vocab)을 참고하세요.

## **코드4** BERT 토크나이저 선언
{: .no_toc .text-delta } 
```python
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(save_path, do_lower_case=False)
```

예시 문장 세 개를 토큰화하는 코드는 코드5입니다. 그 결과는 표4와 같습니다. 토큰 일부에 있는 `##`은 해당 토큰이 어절(띄어쓰기 기준)의 시작이 아님을 나타냅니다. 예컨대 `##네요`는 이 토큰이 앞선 토큰 `짜증나`와 같은 어절에 위치하며 어절 내에서 연속되고 있음을 표시합니다.

## **코드5** BERT 토크나이저로 토큰화하기
{: .no_toc .text-delta } 
```python
sentences = [
    "아 더빙.. 진짜 짜증나네요 목소리",
    "흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나",
    "별루 였다..",
]
tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]
```

## **표4** BERT 토크나이저 토큰화 결과
{: .no_toc .text-delta } 

|구분|토큰1|토큰2|토큰3|토큰4|토큰5|토큰6|토큰7|토큰8|토큰9|토큰10|토큰11|토큰12|토큰13|토큰14|토큰15|토큰16|토큰17|토큰18|토큰19|토큰20|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|문장1|아|더빙.|.|진짜|짜증나|##네요|목소리|
|문장2|흠|.|.|.|포스터|##보고|초딩|##영화|##줄|.|.|.|.|오버|##연기|##조차|가볍|##지|않|##구나|
|문장3|별루|였다|.|.|

코드5와 표4는 BERT 토크나이저의 토큰화 결과를 살짝 맛보기 위해 설명한 것인데요. 실제 모델 입력값은 코드6으로 만듭니다.

## **코드6** BERT 모델 입력 만들기
{: .no_toc .text-delta } 
```python
batch_inputs = tokenizer(
    sentences,
    padding="max_length",
    max_length=12,
    truncation=True,
)
```

코드6 실행 결과로 세 가지의 입력값이 만들어집니다. 하나는 `input_ids`입니다. 표5의 `input_ids`는 표4의 토큰화 결과를 가지고 각 토큰들을 인덱스(index)로 바꾼 것입니다. 어휘 집합(`vocab.txt`)에서 그 순서를 참고해 순서를 인덱스로 변환한 형태입니다. 예컨대 `환상`이라는 토큰이 어휘 집합에서 3357번째로 등장했더면 해당 토큰의 인덱스는 3357이 됩니다.

## **표5** BERT input_ids
{: .no_toc .text-delta } 

|구분|토큰1|토큰2|토큰3|토큰4|토큰5|토큰6|토큰7|토큰8|토큰9|토큰10|토큰11|토큰12|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|문장1|2|621|2631|16|16|1993|3678|1990|3323|3|0|0|
|문장2|2|997|16|16|16|2609|2045|2796|1981|1517|16|3|
|문장3|2|3274|9507|16|16|3|0|0|0|0|0|0|

표5를 자세히 보시면 모든 문장 앞에 2, 끝에 3이 덧붙여진 걸 확인할 수 있습니다. 이는 각각 `[CLS]`, `[SEP]`라는 토큰에 대응하는 인덱스인데요. BERT는 문장 시작과 끝에 이 두 개 토큰을 덧붙이는 특징이 있습니다.

아울러 모든 문장의 길이가 12로 맞춰진걸 볼 수 있습니다. 코드6에서 `max_length` 인자에 12를 넣었기 때문인데요. 이보다 짧은 문장1과 문장3은 뒤에 `[PAD]` 토큰에 해당하는 인덱스 0이 덧붙여져 있습니다. `[PAD]` 토큰은 일종의 더미 토큰으로 길이를 맞춰주는 역할을 합니다. 문장2는 원래 토큰 길이가 20이었는데 12로 줄었습니다. `truncation` 옵션을 `True`로 줬기 때문입니다.

`attention_mask`도 만들어졌습니다. BERT의 `attention_mask`는 GPT와 마찬가지로 일반 토큰이 자리한 곳(=1)과 패딩 토큰이 자리한 곳(=0)을 구분해 알려주는 장치입니다. 표7과 같습니다.

## **표7** BERT attention_mask
{: .no_toc .text-delta } 

|구분|토큰1|토큰2|토큰3|토큰4|토큰5|토큰6|토큰7|토큰8|토큰9|토큰10|토큰11|토큰12|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|문장1|1|1|1|1|1|1|1|1|1|1|0|0|
|문장2|1|1|1|1|1|1|1|1|1|1|1|1|
|문장3|1|1|1|1|1|1|0|0|0|0|0|0|

한편 코드6 실행 결과에 `token_type_ids`이라는 입력값도 있습니다. 이는 세그먼트(segment)에 해당하는 것으로 모두 0입니다.

---